---
title: "Class09"
author: "Joseph Herdy"
date: "2/5/2020"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### K-Means Clustering

#trying some kmeans() functions in R to cluster some made up example data

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```


Use kmeans function setting k to 2 and nstart=20
```{r}
km <- kmeans(x, centers = 2, nstart = 20)

km
```

what is in object km?

```{r}
attributes(km)
```


whats the size of the kmeans?
```{r}
km$size
```

what are the cluser assignment/membership? 
```{r}
km$cluster
```

let's check how many 2's and 1's there are in this vector with the table() function

```{r}
table(km$cluster)
```


plot x colored by the kmeans cluster assignment and add cluster centers as bluer points

```{r}
plot(x, col=km$cluster) #plot the kmeans, color by cluster (2 clusters)
points(km$centers, col = "blue", pch = 15) #add in the point to label center of cluster
```



## Hierarchical clustering in R

hclust() boiiiiiiiiii. Main hier clustering method in R, must be passed a distance matrix as input, not your raw data!

```{r}
hc <- hclust(dist(x))
hc

```


```{r}
plot(hc) 
abline(h=6, col = "red")
```

You can cut the tree at a given height to get the number of clusters you want
```{r}
cutree(hc, h=6)
```



```{r}
table(cutree(hc, h=3.5))
```


You can also cutree with a given number of 'k' groups
```{r}
cutree(hc, k = 5)
table(cutree(hc, k = 5))
```


```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd=0.3), ncol = 2), # c1
 matrix(rnorm(100, mean=1, sd=0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean=1, sd=0.3), # c3
 rnorm(50, mean=0, sd=0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree()
 functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

```{r}
hc_diy <- hclust(dist(x))
plot(hc_diy)

grp3 <- cutree(hc_diy, k =3)
table(grp3)

```


```{r}
plot(x, col=grp3) #color points by the cutree cluster assignments
```




## Hands on PCA english food biz

```{r}
x <- read.csv("UK_foods.csv",row.names = 1)
dim(x)
x
```

Make some conventional plots

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```


```{r}
pairs(x, col=rainbow(10), pch=16)

```

PCA time, bish

```{r}
pca <- prcomp( t(x))
plot(pca)
```

```{r}
summary(pca)
```
```{r}
attributes(pca)
```

```{r}
plot(pca$x[,1], pca$x[,2])
text(pca$x[,1], pca$x[,2], labels = colnames(x),
     col=c("black", "red", "blue", "darkgreen"))
```












